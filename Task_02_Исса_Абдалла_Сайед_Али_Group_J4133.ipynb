{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/n9HTknyjUkI68iSa+MgO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbdAllAh950/Machine-Learning/blob/main/Task_02_%D0%98%D1%81%D1%81%D0%B0_%D0%90%D0%B1%D0%B4%D0%B0%D0%BB%D0%BB%D0%B0_%D0%A1%D0%B0%D0%B9%D0%B5%D0%B4_%D0%90%D0%BB%D0%B8_Group_J4133.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "import requests"
      ],
      "metadata": {
        "id": "72xw1ae-uu81"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary nltk data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQXPnVCl5JIc",
        "outputId": "3f2848d5-3b12-4abf-be61-fc84fd046fa6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load text from Project Gutenberg\n",
        "url = 'http://www.gutenberg.org/files/11/11-0.txt'\n",
        "text = requests.get(url).text"
      ],
      "metadata": {
        "id": "UY-lw26t5KGZ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess text: lowercasing, removing non-alphabet characters, and lemmatizing\n",
        "def preprocess(text):\n",
        "    text = text.lower()  # Lowercase\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove non-alphabet characters\n",
        "    words = nltk.word_tokenize(text)  # Tokenize\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "wm88YzNn5L1i"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the text into chapters (first 12 chapters only)\n",
        "chapters = text.split(\"CHAPTER \")[1:13]\n",
        "processed_chapters = [preprocess(chapter) for chapter in chapters]\n"
      ],
      "metadata": {
        "id": "Zkws2NEH5N7Q"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find top 10 words by TF-IDF for each chapter and generate chapter titles\n",
        "tfidf = TfidfVectorizer(max_features=10)\n",
        "chapter_titles = []\n",
        "for i, chapter in enumerate(processed_chapters):\n",
        "    tfidf.fit([chapter])\n",
        "    top_words = tfidf.get_feature_names_out()\n",
        "    title = f\"Chapter {i+1} - {', '.join(top_words)}\"\n",
        "    chapter_titles.append(title)"
      ],
      "metadata": {
        "id": "thMiH5aw5PxC"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display chapter titles\n",
        "for title in chapter_titles:\n",
        "    print(title)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_KShpLr5RRZ",
        "outputId": "1adc2798-91f5-44f0-e599-2c3c47f37275"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter 1 - rabbithole\n",
            "Chapter 2 - ii, pool, tear\n",
            "Chapter 3 - caucusrace, iii, long, tale\n",
            "Chapter 4 - bill, iv, little, rabbit, sends\n",
            "Chapter 5 - advice, caterpillar\n",
            "Chapter 6 - pepper, pig, vi\n",
            "Chapter 7 - mad, teaparty, vii\n",
            "Chapter 8 - croquetground, queen, viii\n",
            "Chapter 9 - ix, mock, story, turtle\n",
            "Chapter 10 - lobster, quadrille\n",
            "Chapter 11 - stole, tart, xi\n",
            "Chapter 12 - alices, evidence, xii\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract sentences containing \"Alice\" and find most common verbs\n",
        "alice_sentences = [sent for sent in nltk.sent_tokenize(text) if \"alice\" in sent.lower()]\n",
        "verbs = []\n",
        "for sentence in alice_sentences:\n",
        "    pos_tags = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
        "    # Select only verbs from POS tags, filtering out punctuation and contractions\n",
        "    verbs += [word for word, tag in pos_tags if tag.startswith('VB') and word.isalpha()]\n"
      ],
      "metadata": {
        "id": "14gpSL1F5TRg"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find and display top 10 verbs associated with Alice\n",
        "top_verbs = nltk.FreqDist(verbs).most_common(10)\n",
        "print(\"\\nTop 10 verbs associated with Alice:\")\n",
        "for verb, freq in top_verbs:\n",
        "    print(f\"{verb}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtZIndWr5U9h",
        "outputId": "b1976921-a84f-4bb9-87a2-9088fd93af5a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 10 verbs associated with Alice:\n",
            "said: 254\n",
            "was: 174\n",
            "had: 94\n",
            "be: 81\n",
            "s: 50\n",
            "thought: 50\n",
            "have: 41\n",
            "went: 41\n",
            "were: 41\n",
            "is: 39\n"
          ]
        }
      ]
    }
  ]
}