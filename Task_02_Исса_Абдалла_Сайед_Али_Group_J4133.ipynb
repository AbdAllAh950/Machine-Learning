{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYsqx+IesVm7Q13uUctWzO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbdAllAh950/Machine-Learning/blob/main/Task_02_%D0%98%D1%81%D1%81%D0%B0_%D0%90%D0%B1%D0%B4%D0%B0%D0%BB%D0%BB%D0%B0_%D0%A1%D0%B0%D0%B9%D0%B5%D0%B4_%D0%90%D0%BB%D0%B8_Group_J4133.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting Up the Environment**"
      ],
      "metadata": {
        "id": "WEV54fadxr5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import requests\n",
        "import nltk\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Download the text from Project Gutenberg\n",
        "url = \"http://www.gutenberg.org/files/11/11-0.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wvac8LzPxrrQ",
        "outputId": "2e12732f-ce6d-4026-83d7-975a14e6c0e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocess the Text**\\\n",
        "Perform preprocessing steps: convert to lowercase, remove non-alphabetic characters, remove stop words, and lemmatize."
      ],
      "metadata": {
        "id": "Qs5niHtcxzxr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcUarulItbH1"
      },
      "outputs": [],
      "source": [
        "# Initialize lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Preprocess function\n",
        "def preprocess(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove non-alphabetic characters\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    # Tokenize words\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stopwords and lemmatize\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    return words\n",
        "\n",
        "# Preprocess the entire text\n",
        "processed_text = preprocess(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Find the Top 10 Most Important Words in Each Chapter**\\\n",
        "1- Split the text into chapters: Use \"CHAPTER\" as a separator since the text uses it to mark new chapters.\\\n",
        "2- Calculate TF-IDF for each chapter and identify the top 10 most important words."
      ],
      "metadata": {
        "id": "-WRlYElvx_jb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Split the text into chapters\n",
        "chapters = text.split(\"CHAPTER\")[1:]  # Skip any intro part\n",
        "\n",
        "# Initialize the TF-IDF vectorizer, using 'english' stop words\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=10, stop_words='english')\n",
        "\n",
        "# Function to find top TF-IDF words in each chapter\n",
        "def get_top_words_per_chapter(chapter_text):\n",
        "    # Fit and transform the text for each chapter\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform([chapter_text])\n",
        "    # Get feature names (words) and their scores\n",
        "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "    scores = tfidf_matrix.toarray().flatten()\n",
        "    # Get top 10 words with highest scores\n",
        "    top_words = [word for word, score in sorted(zip(feature_names, scores), key=lambda x: x[1], reverse=True)]\n",
        "    return top_words\n",
        "\n",
        "# Process each chapter and print the top 10 words\n",
        "for i, chapter in enumerate(chapters):\n",
        "    # Preprocess the chapter text\n",
        "    processed_chapter = \" \".join(preprocess(chapter))\n",
        "    # Get top words\n",
        "    top_words = get_top_words_per_chapter(processed_chapter)\n",
        "    print(f\"Top 10 words for Chapter {i + 1}: {top_words}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJHZAfRXyM-j",
        "outputId": "c28ec95b-bc2e-4097-97bb-569f9a05bff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 words for Chapter 1: ['rabbithole']\n",
            "Top 10 words for Chapter 2: ['ii', 'pool', 'tear']\n",
            "Top 10 words for Chapter 3: ['caucusrace', 'iii', 'long', 'tale']\n",
            "Top 10 words for Chapter 4: ['iv', 'little', 'rabbit', 'sends']\n",
            "Top 10 words for Chapter 5: ['advice', 'caterpillar']\n",
            "Top 10 words for Chapter 6: ['pepper', 'pig', 'vi']\n",
            "Top 10 words for Chapter 7: ['mad', 'teaparty', 'vii']\n",
            "Top 10 words for Chapter 8: ['croquetground', 'queen', 'viii']\n",
            "Top 10 words for Chapter 9: ['ix', 'mock', 'story', 'turtle']\n",
            "Top 10 words for Chapter 10: ['lobster', 'quadrille']\n",
            "Top 10 words for Chapter 11: ['stole', 'tart', 'xi']\n",
            "Top 10 words for Chapter 12: ['alices', 'evidence', 'xii']\n",
            "Top 10 words for Chapter 13: ['alice', 'little', 'like', 'think', 'way', 'door', 'said', 'thought', 'time', 'went']\n",
            "Top 10 words for Chapter 14: ['alice', 'little', 'mouse', 'im', 'said', 'dear', 'foot', 'thing', 'like', 'went']\n",
            "Top 10 words for Chapter 15: ['said', 'alice', 'mouse', 'dodo', 'know', 'soon', 'bird', 'dry', 'prize', 'round']\n",
            "Top 10 words for Chapter 16: ['alice', 'little', 'rabbit', 'said', 'heard', 'sure', 'thought', 'came', 'thing', 'window']\n",
            "Top 10 words for Chapter 17: ['said', 'alice', 'caterpillar', 'im', 'pigeon', 'serpent', 'little', 'ive', 'minute', 'think']\n",
            "Top 10 words for Chapter 18: ['said', 'alice', 'cat', 'like', 'duchess', 'little', 'footman', 'baby', 'know', 'mad']\n",
            "Top 10 words for Chapter 19: ['said', 'alice', 'hatter', 'dormouse', 'hare', 'march', 'time', 'know', 'thing', 'went']\n",
            "Top 10 words for Chapter 20: ['said', 'alice', 'queen', 'head', 'king', 'cat', 'hedgehog', 'like', 'went', 'soldier']\n",
            "Top 10 words for Chapter 21: ['said', 'alice', 'turtle', 'mock', 'gryphon', 'duchess', 'queen', 'went', 'dont', 'little']\n",
            "Top 10 words for Chapter 22: ['said', 'gryphon', 'turtle', 'alice', 'mock', 'lobster', 'dance', 'beautiful', 'soup', 'voice']\n",
            "Top 10 words for Chapter 23: ['said', 'king', 'hatter', 'alice', 'court', 'dormouse', 'witness', 'queen', 'began', 'thought']\n",
            "Top 10 words for Chapter 24: ['said', 'alice', 'king', 'jury', 'little', 'queen', 'know', 'head', 'rabbit', 'white']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Find the Top 10 Most Used Verbs in Sentences with \"Alice\"**\\\n",
        "1- Find sentences that mention \"Alice\".\\\n",
        "2- Extract verbs from those sentences.\\\n",
        "3- Count the most common verbs and print the top 10."
      ],
      "metadata": {
        "id": "nZ5o0DRZzLlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download parts of speech tagger\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Tokenize the text into sentences\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "# Function to extract verbs from sentences mentioning Alice\n",
        "def get_verbs_with_alice(sentences):\n",
        "    verbs = []\n",
        "    for sentence in sentences:\n",
        "        if 'alice' in sentence.lower():\n",
        "            # Tokenize and tag parts of speech\n",
        "            words = word_tokenize(sentence)\n",
        "            pos_tags = nltk.pos_tag(words)\n",
        "            # Extract verbs, lemmatize them, and add to list\n",
        "            verbs += [lemmatizer.lemmatize(word.lower()) for word, tag in pos_tags if tag.startswith('VB')]\n",
        "    return verbs\n",
        "\n",
        "# Get verbs from sentences with \"Alice\" and find the top 10\n",
        "alice_verbs = get_verbs_with_alice(sentences)\n",
        "top_alice_verbs = Counter(alice_verbs).most_common(10)\n",
        "print(\"Top 10 verbs associated with Alice:\", top_alice_verbs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0w2EFlEAzTvi",
        "outputId": "c1ca380e-4cab-402a-aebb-6ced7b9bd10d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 verbs associated with Alice: [('said', 254), ('wa', 174), ('’', 152), ('had', 94), ('“', 91), ('be', 81), ('s', 50), ('thought', 50), ('”', 48), ('have', 43)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentence Extraction:** We first split the text into sentences and check each one for the word \"Alice\".\\\n",
        "**Verb Extraction**: For each sentence that mentions \"Alice\", we tokenize and tag each word’s part of speech, selecting only verbs.\\\n",
        "**Counting and Displaying Verbs:** We use Counter to find the 10 most common verbs in sentences mentioning \"Alice\"."
      ],
      "metadata": {
        "id": "Uot05Y_EzY_q"
      }
    }
  ]
}